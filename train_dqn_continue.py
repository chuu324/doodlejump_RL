"""
DQNç»§ç»­è®­ç»ƒè„šæœ¬
ä»600å›åˆçš„æœ€ä½³æ¨¡å‹ç»§ç»­è®­ç»ƒï¼Œä½¿ç”¨è°ƒæ•´åçš„è¶…å‚æ•°
"""

import numpy as np
import torch
from collections import deque
import matplotlib.pyplot as plt
import dqn_model
from dqn_model import DQNAgent
from rl_env import DoodleJumpEnv
import time
import os

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# ========== ç»§ç»­è®­ç»ƒçš„è¶…å‚æ•°é…ç½® ==========

# è®­ç»ƒé…ç½®
START_EPISODE = 601        # ä»601å›åˆå¼€å§‹
NUM_EPISODES = 900         # è®­ç»ƒåˆ°900å›åˆ
CONTINUE_FROM_MODEL = 'models/dqn_best.pth'  # åŠ è½½çš„æ¨¡å‹è·¯å¾„

# æ¢ç´¢ç‡ç­–ç•¥ (å…³é”®è°ƒæ•´)
EPSILON_START = 0.20       # é‡ç½®æ¢ç´¢ç‡åˆ°20%
EPSILON_END = 0.10         # æœ€ä½10%(é¿å…è¿‡åº¦åˆ©ç”¨)
EPSILON_DECAY = 0.9992     # ææ…¢è¡°å‡(åŸ0.998)

# å­¦ä¹ ç‡
LEARNING_RATE = 1e-4       # é™ä½å­¦ä¹ ç‡(åŸ3e-4)

# æ›´æ–°ç­–ç•¥
UPDATE_EVERY = 3           # æ¯3æ­¥æ›´æ–°(åŸ2,æ›´ç¨³å®š)
TAU = 0.005                # ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°ç³»æ•°(åŸ0.003,åŠ å¿«åŒæ­¥)

# æ‰¹æ¬¡å¤§å°
BATCH_SIZE = 128           # ä¿æŒ

# ç¼“å†²åŒº
BUFFER_SIZE = 200000       # ä¿æŒ

# æŠ˜æ‰£å› å­
GAMMA = 0.95               # ä¿æŒ

# æ¢¯åº¦è£å‰ª
GRADIENT_CLIP = 0.5        # ä¿æŒ

# è®­ç»ƒå‚æ•°
MAX_STEPS = 20000          # æ¯ä¸ªå›åˆæœ€å¤§æ­¥æ•°
SCORE_WINDOW = 100         # è®¡ç®—å¹³å‡åˆ†æ•°çš„çª—å£å¤§å°
SAVE_INTERVAL = 50         # æ¯éš”å¤šå°‘å›åˆä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
BEST_CHECK_INTERVAL = 50   # æ¯éš”å¤šå°‘å›åˆæ£€æŸ¥ä¸€æ¬¡æœ€ä½³å¹³å‡åˆ†
PRINT_INTERVAL = 10        # æ¯éš”å¤šå°‘å›åˆæ‰“å°ä¸€æ¬¡ä¿¡æ¯

# æ¨¡å‹ä¿å­˜è·¯å¾„
MODEL_DIR = "models/continue"
CONTINUE_CHECKPOINT_PREFIX = "continue_checkpoint_ep"
CONTINUE_BEST_PREFIX = "continue_best_ep"


def train_continue():
    """ä»å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒDQNæ™ºèƒ½ä½“"""
    
    # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
    os.makedirs(MODEL_DIR, exist_ok=True)
    
    # ========== è¦†ç›–æ¨¡å—çº§å¸¸é‡ ==========
    # ä¸´æ—¶ä¿®æ”¹dqn_modelæ¨¡å—ä¸­çš„å¸¸é‡ï¼Œä»¥ä¾¿åœ¨agentä¸­ä½¿ç”¨æ–°çš„è¶…å‚æ•°
    original_update_every = dqn_model.UPDATE_EVERY
    original_tau = dqn_model.TAU
    original_gamma = dqn_model.GAMMA
    dqn_model.UPDATE_EVERY = UPDATE_EVERY
    dqn_model.TAU = TAU
    dqn_model.GAMMA = GAMMA
    
    # åˆ›å»ºç¯å¢ƒï¼ˆä¸ä½¿ç”¨æ¸²æŸ“ï¼ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼‰
    env = DoodleJumpEnv(render_mode=None)
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    state_size = env.observation_space.shape[0]  # 27 (ä½¿ç”¨å‘¨æœŸæ€§ç¼–ç å)
    action_size = env.action_space.n  # 3
    agent = DQNAgent(state_size=state_size, action_size=action_size)
    
    # ========== åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹ ==========
    print(f"\n{'='*80}")
    print(f"åŠ è½½æ¨¡å‹: {CONTINUE_FROM_MODEL}")
    if not agent.load(CONTINUE_FROM_MODEL):
        print(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹: {CONTINUE_FROM_MODEL}")
        print("è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨ï¼")
        # æ¢å¤åŸå§‹å¸¸é‡
        dqn_model.UPDATE_EVERY = original_update_every
        dqn_model.TAU = original_tau
        dqn_model.GAMMA = original_gamma
        return None, None
    
    # è¦†ç›–æ¢ç´¢ç‡(é‡ç½®åˆ°EPSILON_START)
    agent.epsilon = EPSILON_START
    agent.epsilon_decay = EPSILON_DECAY
    agent.epsilon_min = EPSILON_END
    print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"  é‡ç½®æ¢ç´¢ç‡: {agent.epsilon:.3f}")
    print(f"  æ¢ç´¢ç‡è¡°å‡: {EPSILON_DECAY}")
    print(f"  æœ€ä½æ¢ç´¢ç‡: {EPSILON_END}")
    
    # è°ƒæ•´å­¦ä¹ ç‡
    for param_group in agent.optimizer.param_groups:
        param_group['lr'] = LEARNING_RATE
    print(f"  è®¾ç½®å­¦ä¹ ç‡: {LEARNING_RATE}")
    
    # é‡ç½®æŸå¤±è®°å½•
    agent.losses = []
    
    # æ˜¾ç¤ºè¶…å‚æ•°é…ç½®
    print(f"\nè¶…å‚æ•°é…ç½®:")
    print(f"  UPDATE_EVERY: {UPDATE_EVERY}")
    print(f"  TAU: {TAU}")
    print(f"  GAMMA: {GAMMA}")
    print(f"  BATCH_SIZE: {BATCH_SIZE}")
    print(f"  BUFFER_SIZE: {BUFFER_SIZE}")
    print(f"{'='*80}\n")
    
    # è®­ç»ƒç»Ÿè®¡
    scores = []  # æ¯ä¸ªå›åˆçš„åˆ†æ•°
    scores_window = deque(maxlen=SCORE_WINDOW)  # æœ€è¿‘100å›åˆçš„åˆ†æ•°
    best_score = -np.inf  # å•ä¸ªå›åˆæœ€ä½³åˆ†æ•°
    best_mean_score = -np.inf  # å†å²æœ€ä½³å¹³å‡åˆ†æ•°
    display_best_mean_score = -np.inf  # æ˜¾ç¤ºç”¨çš„æœ€ä½³å¹³å‡åˆ†æ•°
    episode_durations = []  # æ¯ä¸ªå›åˆçš„æŒç»­æ—¶é—´
    
    print("=" * 80)
    print("å¼€å§‹ç»§ç»­è®­ç»ƒDQNæ™ºèƒ½ä½“")
    print(f"è®­ç»ƒå›åˆ: {START_EPISODE} â†’ {NUM_EPISODES}")
    print(f"çŠ¶æ€ç©ºé—´ç»´åº¦: {state_size}")
    print(f"åŠ¨ä½œç©ºé—´å¤§å°: {action_size}")
    # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
    if torch.cuda.is_available():
        print(f"âœ“ ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
        print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        print("âš  ä½¿ç”¨CPUï¼ˆå»ºè®®ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒï¼‰")
    print("=" * 80)
    
    start_time = time.time()
    
    for episode in range(START_EPISODE, NUM_EPISODES + 1):
        # æ›´æ–°æ™ºèƒ½ä½“çš„å½“å‰å›åˆæ•°ï¼ˆç”¨äºåˆ†é˜¶æ®µæ¢¯åº¦è£å‰ªï¼‰
        agent.current_episode = episode
        
        # é‡ç½®ç¯å¢ƒ
        state, info = env.reset()
        score = 0
        episode_start_time = time.time()
        
        for step in range(MAX_STEPS):
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.act(state, training=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, info = env.step(action)
            
            # ä¿å­˜ç»éªŒå¹¶å­¦ä¹ 
            agent.step(state, action, reward, next_state, terminated)
            
            # æ›´æ–°çŠ¶æ€
            state = next_state
            score = info['score']
            
            # å¦‚æœæ¸¸æˆç»“æŸï¼Œè·³å‡ºå¾ªç¯
            if terminated or truncated:
                break
        
        # æ›´æ–°æ¢ç´¢ç‡ï¼ˆä½¿ç”¨æ–°çš„è¡°å‡ç­–ç•¥ï¼‰
        agent.epsilon = max(EPSILON_END, agent.epsilon * EPSILON_DECAY)
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        scores.append(score)
        scores_window.append(score)
        episode_duration = time.time() - episode_start_time
        episode_durations.append(episode_duration)
        
        # æ›´æ–°å•ä¸ªå›åˆæœ€ä½³åˆ†æ•°
        if score > best_score:
            best_score = score
        
        # è®¡ç®—å½“å‰å¹³å‡åˆ†æ•°ï¼ˆç”¨äºæ˜¾ç¤ºå’Œæ£€æŸ¥ï¼‰
        mean_score = np.mean(scores_window) if len(scores_window) > 0 else 0
        
        # æ¯10å›åˆæ›´æ–°æ˜¾ç¤ºç”¨çš„æœ€ä½³å¹³å‡åˆ†æ•°
        if episode % PRINT_INTERVAL == 0 and len(scores_window) >= BEST_CHECK_INTERVAL:
            if mean_score > display_best_mean_score:
                display_best_mean_score = mean_score
        
        # åœ¨ç¬¬ä¸€æ¬¡æœ‰è¶³å¤Ÿæ•°æ®æ—¶åˆå§‹åŒ–æœ€ä½³å¹³å‡åˆ†æ•°å¹¶ä¿å­˜æ¨¡å‹
        if best_mean_score == -np.inf and len(scores_window) >= BEST_CHECK_INTERVAL:
            best_mean_score = mean_score
            display_best_mean_score = mean_score
            best_model_path = os.path.join(MODEL_DIR, f"{CONTINUE_BEST_PREFIX}{episode}_score{mean_score:.1f}.pth")
            agent.save(best_model_path)
            print(f"\nğŸ‰ åˆå§‹åŒ–æœ€ä½³å¹³å‡åˆ†æ•°: {best_mean_score:.1f} (å›åˆ {episode}, æœ€è¿‘{len(scores_window)}å›åˆå¹³å‡)")
        
        # æ¯50å›åˆæ£€æŸ¥ä¸€æ¬¡å¹³å‡åˆ†ï¼Œå¦‚æœè¶…è¿‡å†å²æœ€ä½³å¹³å‡åˆ†åˆ™ä¿å­˜ä¸ºæœ€ä½³æ¨¡å‹
        best_model_saved = False
        if episode % BEST_CHECK_INTERVAL == 0 and len(scores_window) >= BEST_CHECK_INTERVAL:
            if mean_score > best_mean_score:
                best_mean_score = mean_score
                display_best_mean_score = mean_score
                best_model_path = os.path.join(MODEL_DIR, f"{CONTINUE_BEST_PREFIX}{episode}_score{mean_score:.1f}.pth")
                agent.save(best_model_path)
                best_model_saved = True
                print(f"\nğŸ‰ æ–°çš„æœ€ä½³å¹³å‡åˆ†æ•°: {best_mean_score:.1f} (å›åˆ {episode}, æœ€è¿‘{len(scores_window)}å›åˆå¹³å‡)")
        
        # ========== æ€§èƒ½ä¸‹é™é¢„è­¦ ==========
        if episode >= START_EPISODE + 100 and len(scores) >= 100:
            recent_50 = np.mean(scores[-50:])
            baseline_100 = np.mean(scores[-100:-50])
            
            if recent_50 < baseline_100 * 0.95:
                print(f"\nâš ï¸  è­¦å‘Š: æ€§èƒ½ä¸‹é™ {baseline_100:.0f} â†’ {recent_50:.0f} (ä¸‹é™ {((baseline_100 - recent_50) / baseline_100 * 100):.1f}%)")
                print(f"   è‡ªåŠ¨æå‡æ¢ç´¢ç‡: {agent.epsilon:.3f} â†’ ", end="")
                agent.epsilon = min(agent.epsilon * 1.15, 0.25)
                print(f"{agent.epsilon:.3f}")
        
        # æ‰“å°è®­ç»ƒä¿¡æ¯
        if episode % PRINT_INTERVAL == 0:
            mean_duration = np.mean(episode_durations[-PRINT_INTERVAL:])
            elapsed_time = time.time() - start_time
            
            # è®¡ç®—å¹³å‡æŸå¤±ï¼ˆæœ€è¿‘1000æ¬¡æ›´æ–°ï¼Œå¦‚æœä¸è¶³1000æ¬¡åˆ™ä½¿ç”¨å…¨éƒ¨ï¼‰
            if len(agent.losses) > 0:
                recent_losses = agent.losses[-1000:] if len(agent.losses) >= 1000 else agent.losses
                avg_loss = np.mean(recent_losses)
                loss_str = f"{avg_loss:.2f}"
            else:
                loss_str = "N/A"
            
            # è·å–å½“å‰å­¦ä¹ ç‡
            current_lr = agent.optimizer.param_groups[0]['lr']
            
            # æ ¼å¼åŒ–æœ€ä½³å¹³å‡åˆ†æ•°æ˜¾ç¤º
            best_mean_display = display_best_mean_score if display_best_mean_score != -np.inf else mean_score
            
            print(f"å›åˆ {episode:4d} | "
                  f"å¹³å‡åˆ†: {mean_score:6.1f} | "
                  f"å½“å‰åˆ†: {score:6.1f} | "
                  f"Îµ: {agent.epsilon:.3f} | "
                  f"Loss: {loss_str:>6} | "
                  f"LR: {current_lr:.6f}")
        
        # å®šæœŸä¿å­˜æ¨¡å‹checkpoint
        if episode % SAVE_INTERVAL == 0:
            checkpoint_path = os.path.join(MODEL_DIR, f"{CONTINUE_CHECKPOINT_PREFIX}{episode}.pth")
            agent.save(checkpoint_path)
            if best_model_saved:
                print(f"Checkpointå·²ä¿å­˜: {checkpoint_path} (åŒæ—¶å·²ä¿å­˜æœ€ä½³æ¨¡å‹)")
            else:
                print(f"Checkpointå·²ä¿å­˜: {checkpoint_path}")
    
    # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_checkpoint_path = os.path.join(MODEL_DIR, f"{CONTINUE_CHECKPOINT_PREFIX}{NUM_EPISODES}_final.pth")
    agent.save(final_checkpoint_path)
    total_time = time.time() - start_time
    
    # æ¢å¤åŸå§‹å¸¸é‡
    dqn_model.UPDATE_EVERY = original_update_every
    dqn_model.TAU = original_tau
    dqn_model.GAMMA = original_gamma
    
    print("\n" + "=" * 80)
    print("ç»§ç»­è®­ç»ƒå®Œæˆ!")
    print(f"  è®­ç»ƒå›åˆ: {START_EPISODE} â†’ {NUM_EPISODES}")
    print(f"  æ€»è®­ç»ƒæ—¶é—´: {total_time/60:.2f} åˆ†é’Ÿ")
    print(f"  æœ€ä½³å•å›åˆåˆ†æ•°: {best_score:.1f}")
    print(f"  æœ€ä½³å¹³å‡åˆ†æ•°: {best_mean_score:.1f}")
    print(f"  æœ€å100å›åˆå¹³å‡åˆ†æ•°: {np.mean(scores_window):.1f}")
    print(f"  æœ€ç»ˆæ¢ç´¢ç‡: {agent.epsilon:.3f}")
    print(f"  æœ€ç»ˆå­¦ä¹ ç‡: {agent.optimizer.param_groups[0]['lr']:.6f}")
    print(f"  æ¨¡å‹ä¿å­˜äº: {MODEL_DIR}/{CONTINUE_BEST_PREFIX}*.pth")
    print("=" * 80)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curve(scores, scores_window, START_EPISODE)
    
    env.close()
    return agent, scores


def plot_training_curve(scores, scores_window, start_episode):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    try:
        plt.figure(figsize=(14, 6))
        
        # å­å›¾1: æ‰€æœ‰å›åˆçš„åˆ†æ•°
        plt.subplot(1, 2, 1)
        episode_numbers = range(start_episode, start_episode + len(scores))
        plt.plot(episode_numbers, scores, alpha=0.3, color='blue', label='æ¯å›åˆåˆ†æ•°')
        if len(scores_window) > 0:
            window_scores = [np.mean(list(scores_window)[:i+1]) for i in range(len(scores_window))]
            window_start = start_episode + len(scores) - len(scores_window)
            plt.plot(range(window_start, window_start + len(window_scores)), 
                    window_scores, color='red', linewidth=2, label=f'å¹³å‡åˆ†æ•° ({SCORE_WINDOW}å›åˆ)')
        plt.xlabel('å›åˆ')
        plt.ylabel('åˆ†æ•°')
        plt.title(f'ç»§ç»­è®­ç»ƒè¿‡ç¨‹ - åˆ†æ•°æ›²çº¿ (å›åˆ {start_episode}-{start_episode + len(scores) - 1})')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å­å›¾2: æœ€è¿‘100å›åˆçš„å¹³å‡åˆ†æ•°
        plt.subplot(1, 2, 2)
        if len(scores_window) > 0:
            window_scores = list(scores_window)
            window_start = start_episode + len(scores) - len(scores_window)
            plt.plot(range(window_start, window_start + len(window_scores)), 
                    window_scores, alpha=0.5, color='blue', label='æœ€è¿‘100å›åˆåˆ†æ•°')
            if len(window_scores) >= 10:
                # ç§»åŠ¨å¹³å‡
                moving_avg = np.convolve(window_scores, np.ones(10)/10, mode='valid')
                plt.plot(range(window_start + 9, window_start + len(window_scores)), 
                        moving_avg, color='red', linewidth=2, label='10å›åˆç§»åŠ¨å¹³å‡')
        plt.xlabel('å›åˆ')
        plt.ylabel('åˆ†æ•°')
        plt.title('æœ€è¿‘100å›åˆåˆ†æ•°è¶‹åŠ¿')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('training_curve_continue.png', dpi=150, bbox_inches='tight')
        print("è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: training_curve_continue.png")
        plt.close()
    except Exception as e:
        print(f"ç»˜åˆ¶è®­ç»ƒæ›²çº¿æ—¶å‡ºé”™: {e}")


if __name__ == "__main__":
    train_continue()

