# Doodle Jump 游戏环境搭建说明

本文档介绍 Doodle Jump 游戏环境的搭建，包括基础游戏逻辑和专为强化学习训练设计的重要特性。

## 目录

1. [环境概述](#环境概述)
2. [基础游戏逻辑](#基础游戏逻辑)
3. [强化学习接口](#强化学习接口)
4. [状态空间设计](#状态空间设计)
5. [动作空间设计](#动作空间设计)
6. [奖励机制](#奖励机制)
7. [环形边界处理](#环形边界处理)
8. [平台生成机制](#平台生成机制)
9. [无头模式支持](#无头模式支持)

---

## 环境概述

`DoodleJumpEnv` 是基于 Gymnasium 接口实现的 Doodle Jump 游戏环境，专为强化学习训练优化。

### 核心特性

- **Gymnasium 标准接口**：完全兼容 Gymnasium API，可与主流 RL 库无缝集成
- **27维状态空间**：包含玩家状态、障碍物信息、多平台信息，使用周期性编码处理环形边界
- **奖励塑形**：精心设计的奖励函数，引导智能体学习安全高效的策略
- **无头模式**：支持无渲染训练，大幅提升训练速度
- **动态难度**：随高度增加自动调整平台稀疏度和障碍物概率

### 文件结构

```
rl_env.py          # Gymnasium 环境封装
game.py            # 核心游戏逻辑
settings.py        # 游戏参数配置
sprites.py         # 游戏精灵（玩家、平台）
```

---

## 基础游戏逻辑

### 游戏机制

Doodle Jump 是一个垂直跳跃游戏，玩家控制角色在平台间跳跃，目标是尽可能向上移动。

#### 玩家控制

- **水平移动**：通过左右移动控制角色水平位置
- **垂直移动**：自动跳跃（踩到平台时触发），受重力影响下落
- **屏幕环绕**：水平方向支持环形边界（从一侧离开会从另一侧出现）

#### 平台类型

1. **普通平台**（蓝色）：基础平台，提供标准跳跃力
2. **奖励平台**（绿色/黄色）：提供更强的跳跃力（1.5倍）
3. **障碍物平台**（红色）：碰撞会导致游戏结束

### 物理系统

#### 运动学参数

```python
PLAYER_ACC = 0.65        # 水平加速度（像素/帧²）
PLAYER_FRICTION = -0.12  # 摩擦力系数（每帧速度衰减12%）
PLAYER_GRAVITY = 0.75    # 重力加速度（像素/帧²）
PLAYER_JUMP = 18         # 跳跃初速度（像素/帧）
```

#### 运动方程

```python
# 水平加速度（受动作和摩擦力影响）
acc.x = PLAYER_ACC * action_direction + vel.x * PLAYER_FRICTION

# 垂直加速度（重力）
acc.y = PLAYER_GRAVITY

# 速度更新
vel += acc

# 位置更新
pos += vel + 0.5 * acc
```

#### 碰撞检测

- **平台碰撞**：仅在下落时检测（`vel.y > 0`）
- **碰撞响应**：
  - 普通平台：`vel.y = -PLAYER_JUMP`
  - 奖励平台：`vel.y = -PLAYER_JUMP * 1.5`
  - 障碍物：游戏结束（`terminated = True`）

#### 屏幕滚动

- **触发条件**：玩家到达屏幕 1/4 高度时
- **滚动距离**：`scroll_dist = abs(vel.y)`
- **分数增加**：`score += scroll_dist`（分数 = 累计向上移动距离）

### 游戏终止条件

1. **掉落死亡**：玩家 Y 坐标超出屏幕底部
2. **碰撞障碍物**：玩家踩到障碍物平台

---

## 强化学习接口

### 环境初始化

```python
from rl_env import DoodleJumpEnv

# 无头模式（训练时使用，不显示窗口）
env = DoodleJumpEnv(render_mode=None)

# 可视化模式（测试时使用）
env = DoodleJumpEnv(render_mode='human')

# RGB数组模式（用于录制视频）
env = DoodleJumpEnv(render_mode='rgb_array')
```

### 标准接口方法

```python
# 重置环境
obs, info = env.reset(seed=42)

# 执行动作
obs, reward, terminated, truncated, info = env.step(action)

# 渲染（可选）
env.render()

# 关闭环境
env.close()
```

---

## 状态空间设计

### 状态维度：27维

状态空间经过精心设计，包含丰富的环境信息，并使用周期性编码处理环形边界。

#### 状态组成

```
1. 玩家基础状态（5维）
   - x_sin, x_cos: 玩家X坐标的周期性编码（sin/cos对）
   - y: 玩家Y坐标（归一化到[0,1]）
   - vel_y: 垂直速度（归一化到[-1,1]）
   - vel_x: 水平速度（归一化到[-1,1]）⭐ 关键信息

2. 最近障碍物信息（4维）
   - dx: 水平相对位置（使用环形距离，归一化）
   - dy: 垂直相对位置（归一化）
   - dist: 归一化距离
   - type: 类型编码（1.0=障碍物，0.0=无障碍物）

3. 最近平台信息（3维）
   - dx: 水平相对位置（使用环形距离，归一化）
   - dy: 垂直相对位置（归一化）
   - type: 类型编码（1.0=奖励平台，0.0=普通平台）

4. 上方5个平台信息（15维）
   - 每个平台包含：dx, dy, type（3维）
   - 按距离排序，最近的5个平台
   - 类型编码：normal=0.0, boost=0.5, obstacle=1.0
```

### 周期性编码

#### 为什么需要周期性编码？

游戏支持水平方向的环形边界（从右侧离开会从左侧出现）。如果直接使用线性 X 坐标，会导致边界不连续问题：
- 玩家在 X=479 和 X=1 时，线性距离很大，但实际位置很近
- 神经网络难以学习这种不连续性

#### 实现方式

```python
def periodic_encode(x, width):
    """将线性x坐标转换为周期性编码(sin/cos对)"""
    normalized = (x % width) / width
    angle = normalized * 2 * np.pi
    return np.sin(angle), np.cos(angle)
```

**优势**：
- 边界连续：X=479 和 X=1 的编码值接近
- 保持距离信息：通过 sin/cos 对可以恢复相对位置
- 神经网络友好：连续函数，易于学习

### 环形距离计算

对于平台和障碍物的相对位置，使用环形距离而非线性距离：

```python
def toroidal_distance(x1, x2, width):
    """计算环形距离（考虑边界环绕）"""
    direct_dist = abs(x2 - x1)
    wrap_dist = width - direct_dist
    return min(direct_dist, wrap_dist)  # 返回较短路径

def toroidal_dx(x1, x2, width):
    """计算环形相对位置，范围在[-width/2, width/2]"""
    direct_dx = x2 - x1
    if direct_dx > width / 2:
        return direct_dx - width  # 通过左边界更近
    elif direct_dx < -width / 2:
        return direct_dx + width  # 通过右边界更近
    else:
        return direct_dx
```

---

## 动作空间设计

### 动作定义

**离散动作空间**：3个动作

```python
action_space = spaces.Discrete(3)

动作0: 向左移动 (LEFT)
动作1: 静止/不移动 (NOOP)
动作2: 向右移动 (RIGHT)
```

### 动作执行

```python
# 在 game_step_for_rl 中执行
if action == 0:  # 向左
    player.acc.x = -PLAYER_ACC
elif action == 1:  # 不动
    player.acc.x = 0  # 受摩擦力影响，逐渐减速
elif action == 2:  # 向右
    player.acc.x = PLAYER_ACC
```

### 动作约束

- **水平方向**：支持屏幕环绕（自动处理）
- **垂直方向**：受重力影响，只能向上跳跃，不能主动向下
- **跳跃条件**：必须踩在平台上才能跳跃（自动触发）

---

## 奖励机制

奖励函数经过精心设计，采用**奖励塑形（Reward Shaping）**技术，引导智能体学习安全高效的策略。

### 奖励组成

#### 1. 基础奖励

```python
# 存活奖励（每步）
reward += 0.05

# 高度奖励（分数增加）
score_diff = score - last_score
reward += score_diff * 0.9  # 缩小尺度，避免与死亡惩罚不平衡
```

#### 2. 障碍物相关奖励塑形

```python
# 接近障碍物惩罚（距离 < 200像素时）
if min_obstacle_dist < danger_threshold:
    danger_factor = 1.0 - (min_obstacle_dist / danger_threshold)
    obstacle_penalty = -0.1 * danger_factor  # 最大惩罚-0.1
    reward += obstacle_penalty

# 成功规避奖励（上一帧很近，这一帧变远）
if last_obstacle_close and min_obstacle_dist > danger_threshold:
    reward += 1.0  # ⭐ 成功规避奖励
```

**设计目的**：
- 提供中间信号，避免稀疏奖励问题
- 鼓励智能体主动规避障碍物
- 奖励成功规避行为，强化学习效果

#### 3. 位置相关奖励

```python
# 接近屏幕底部惩罚（鼓励向上）
bottom_threshold = HEIGHT * 0.8
if player.pos.y > bottom_threshold:
    bottom_penalty = -0.5 * ((player.pos.y - bottom_threshold) / (HEIGHT - bottom_threshold))
    reward += bottom_penalty
```

#### 4. 死亡惩罚

```python
if terminated:
    reward = -100.0  # 死亡惩罚
```

### 奖励特性

- **奖励尺度**：`[-100, +∞]`，死亡惩罚占主导
- **奖励密度**：每步都有奖励（存活奖励 + 其他）
- **奖励塑形**：提供渐进式反馈，避免稀疏奖励
- **长期目标**：最大化累计高度（分数）

---

## 环形边界处理

### 问题背景

游戏支持水平方向的环形边界，玩家从一侧离开会从另一侧出现。这给状态表示带来了挑战。

### 解决方案

#### 1. 周期性编码（玩家位置）

使用 sin/cos 对编码 X 坐标，确保边界连续性。

#### 2. 环形距离计算（相对位置）

对于平台和障碍物的相对位置，使用环形距离而非线性距离。

#### 3. 状态空间中的体现

- 玩家 X 坐标：使用 `(x_sin, x_cos)` 周期性编码
- 平台/障碍物相对位置：使用 `toroidal_dx()` 计算环形相对位置
- 距离计算：使用 `toroidal_distance()` 计算环形距离

### 优势

- **边界连续**：神经网络可以学习边界附近的策略
- **信息完整**：保持所有必要的空间信息
- **训练稳定**：避免边界不连续导致的训练不稳定

---

## 平台生成机制

### 生成规则

#### 1. 生成时机

- **触发条件**：平台数量 < 目标数量
- **目标数量**：根据稀疏因子动态调整（3-8个）

#### 2. 生成位置约束

**高度限制**：
- 预生成区域：`[highest_y - 150, highest_y - 50]`
- 可见区域：`[-50, player.y + HEIGHT*0.2]`
- **禁止区域**：`y > max_height`（已到达区域）

**可达性检查**：
- 基于物理轨迹计算（`calculate_jump_trajectory`）
- 考虑重力、跳跃高度、水平移动距离
- 最大跳跃高度：`MAX_JUMP_HEIGHT = 36` 像素
- 最大水平距离：`MAX_HORIZONTAL_REACH = 288` 像素

**间距控制**：
- 基础间距：`[30, 120]` 像素
- 动态调整：随高度增加，间距增大（重叠区域减小）

#### 3. 平台类型概率

```python
# 基础概率
BASE_NORMAL_PROB = 0.85   # 普通平台
BASE_BOOST_PROB = 0.1     # 奖励平台
BASE_OBSTACLE_PROB = 0.05 # 障碍物（初始）

# 动态调整
# 1. 稀疏因子影响（normal和boost）
adjusted_normal = BASE_NORMAL_PROB * sparse_factor
adjusted_boost = BASE_BOOST_PROB * sparse_factor

# 2. 障碍物概率随高度增加
obstacle_prob = f(score)  # 0.05 -> 0.15 (分数1000-8000)

# 3. 归一化确保总和=1
```

#### 4. 难度递增机制

**平台稀疏度**：
- 分数 < 2000：不稀疏
- 分数 2000-10000：线性稀疏
- 分数 ≥ 10000：稀疏因子 = 0.5

**障碍物概率**：
- 分数 < 1000：5%
- 分数 1000-8000：5% → 15%
- 分数 ≥ 8000：15%

**平台间距**：
- 分数 < 1000：基础间距
- 分数 1000-10000：间距逐渐增大
- 分数 ≥ 10000：间距增加30%

#### 5. 安全约束

**障碍物安全检查**：
- 与玩家距离 ≥ 140像素
- 不在玩家下落路径上
- 可见区域内最多3个（低高度时最多1个）
- 确保上方有可通行路径

**水平重叠控制**：
- 普通/奖励平台：最大重叠30%
- 障碍物：无重叠限制

---

## 无头模式支持

### 什么是无头模式？

无头模式（Headless Mode）是指不显示游戏窗口的训练模式，可以大幅提升训练速度。

### 实现方式

```python
# 无头模式初始化
if render_mode is None:
    # Linux/Mac: 使用dummy驱动
    if os.name != 'nt':
        os.environ['SDL_VIDEODRIVER'] = 'dummy'
    
    # Windows: 关闭display
    if hasattr(self.game, 'screen') and self.game.screen is not None:
        pygame.display.quit()
        self.game.screen = None
```

### 使用建议

- **训练时**：使用 `render_mode=None`，提升训练速度
- **测试时**：使用 `render_mode='human'`，可视化游戏过程
- **录制视频**：使用 `render_mode='rgb_array'`，获取RGB数组

### 性能提升

无头模式可以提升训练速度 **5-10倍**，特别是在GPU训练时效果显著。

---

## 关键参数速查表

| 参数类别 | 参数名 | 值 | 说明 |
|---------|--------|-----|------|
| **屏幕** | WIDTH | 480 | 屏幕宽度（像素） |
| | HEIGHT | 600 | 屏幕高度（像素） |
| | FPS | 60 | 帧率 |
| **玩家** | PLAYER_ACC | 0.65 | 水平加速度 |
| | PLAYER_FRICTION | -0.12 | 摩擦力系数 |
| | PLAYER_GRAVITY | 0.75 | 重力加速度 |
| | PLAYER_JUMP | 18 | 跳跃初速度 |
| **平台** | PLATFORM_WIDTH | 100 | 平台宽度 |
| | PLATFORM_HEIGHT | 20 | 平台高度 |
| | MAX_JUMP_HEIGHT | 36 | 最大跳跃高度 |
| | MAX_HORIZONTAL_REACH | 288 | 最大水平距离 |
| **难度** | HEIGHT_SPARSE_START | 2000 | 开始稀疏的分数 |
| | HEIGHT_SPARSE_LIMIT | 10000 | 极限稀疏的分数 |
| | OBSTACLE_PROB_START | 1000 | 障碍物开始增加的分数 |
| | OBSTACLE_PROB_LIMIT | 8000 | 障碍物最大概率的分数 |
| | SPACING_ADJUST_START | 1000 | 间距开始调整的分数 |
| | SPACING_ADJUST_LIMIT | 10000 | 间距最大调整的分数 |

---

## 总结

Doodle Jump 游戏环境经过精心设计，具备以下特点：

1. **丰富的状态信息**：27维状态空间，包含玩家状态、障碍物信息、多平台信息
2. **环形边界处理**：使用周期性编码和环形距离，确保边界连续性
3. **奖励塑形**：精心设计的奖励函数，引导智能体学习安全高效的策略
4. **动态难度**：随高度增加自动调整平台稀疏度和障碍物概率
5. **无头模式**：支持无渲染训练，大幅提升训练速度

这些特性使得环境非常适合强化学习训练，能够帮助智能体快速学习有效的游戏策略。

