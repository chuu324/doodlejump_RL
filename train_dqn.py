"""
DQNè®­ç»ƒè„šæœ¬
æ ¹æ®RLç¯å¢ƒæ ¸å¿ƒè¦ç´ æ€»ç»“.mdæ–‡æ¡£è¿›è¡Œè®­ç»ƒ
"""

import numpy as np
import torch
from collections import deque
import matplotlib.pyplot as plt
from dqn_model import DQNAgent
from rl_env import DoodleJumpEnv
import time
import os

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# è®­ç»ƒå‚æ•°
N_EPISODES = 600  # è®­ç»ƒå›åˆæ•°
MAX_STEPS = 20000  # æ¯ä¸ªå›åˆæœ€å¤§æ­¥æ•°
SCORE_WINDOW = 100  # è®¡ç®—å¹³å‡åˆ†æ•°çš„çª—å£å¤§å°
SAVE_INTERVAL = 100  # æ¯éš”å¤šå°‘å›åˆä¿å­˜ä¸€æ¬¡æ¨¡å‹
BEST_CHECK_INTERVAL = 50  # æ¯éš”å¤šå°‘å›åˆæ£€æŸ¥ä¸€æ¬¡æœ€ä½³å¹³å‡åˆ†
PRINT_INTERVAL = 10  # æ¯éš”å¤šå°‘å›åˆæ‰“å°ä¸€æ¬¡ä¿¡æ¯

# æ¨¡å‹ä¿å­˜è·¯å¾„
MODEL_DIR = "models"
MODEL_PATH = os.path.join(MODEL_DIR, "dqn_checkpoint.pth")
BEST_MODEL_PATH = os.path.join(MODEL_DIR, "dqn_best.pth")

def train():
    """è®­ç»ƒDQNæ™ºèƒ½ä½“"""
    
    # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
    os.makedirs(MODEL_DIR, exist_ok=True)
    
    # åˆ›å»ºç¯å¢ƒï¼ˆä¸ä½¿ç”¨æ¸²æŸ“ï¼ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼‰
    env = DoodleJumpEnv(render_mode=None)
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    state_size = env.observation_space.shape[0]  # 27 (ä½¿ç”¨å‘¨æœŸæ€§ç¼–ç å)
    action_size = env.action_space.n  # 3
    agent = DQNAgent(state_size=state_size, action_size=action_size)
    
    # è®­ç»ƒç»Ÿè®¡
    scores = []  # æ¯ä¸ªå›åˆçš„åˆ†æ•°
    scores_window = deque(maxlen=SCORE_WINDOW)  # æœ€è¿‘100å›åˆçš„åˆ†æ•°
    best_score = -np.inf  # å•ä¸ªå›åˆæœ€ä½³åˆ†æ•°
    best_mean_score = -np.inf  # å†å²æœ€ä½³å¹³å‡åˆ†æ•°ï¼ˆç”¨äºä¿å­˜æ¨¡å‹ï¼Œæ¯50å›åˆæ›´æ–°ï¼‰
    display_best_mean_score = -np.inf  # æ˜¾ç¤ºç”¨çš„æœ€ä½³å¹³å‡åˆ†æ•°ï¼ˆæ¯10å›åˆæ›´æ–°ï¼‰
    episode_durations = []  # æ¯ä¸ªå›åˆçš„æŒç»­æ—¶é—´
    
    print("=" * 60)
    print("å¼€å§‹è®­ç»ƒDQNæ™ºèƒ½ä½“")
    print(f"çŠ¶æ€ç©ºé—´ç»´åº¦: {state_size}")
    print(f"åŠ¨ä½œç©ºé—´å¤§å°: {action_size}")
    # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
    if torch.cuda.is_available():
        print(f"âœ“ ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
        print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        print("âš  ä½¿ç”¨CPUï¼ˆå»ºè®®ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒï¼‰")
    print("=" * 60)
    
    start_time = time.time()
    
    for episode in range(1, N_EPISODES + 1):
        # æ›´æ–°æ™ºèƒ½ä½“çš„å½“å‰å›åˆæ•°ï¼ˆç”¨äºåˆ†é˜¶æ®µæ¢¯åº¦è£å‰ªï¼‰
        agent.current_episode = episode
        
        # é‡ç½®ç¯å¢ƒ
        state, info = env.reset()
        score = 0
        episode_start_time = time.time()
        
        for step in range(MAX_STEPS):
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.act(state, training=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, info = env.step(action)
            
            # ä¿å­˜ç»éªŒå¹¶å­¦ä¹ 
            agent.step(state, action, reward, next_state, terminated)
            
            # æ›´æ–°çŠ¶æ€
            state = next_state
            score = info['score']
            
            # å¦‚æœæ¸¸æˆç»“æŸï¼Œè·³å‡ºå¾ªç¯
            if terminated or truncated:
                break
        
        # æ›´æ–°æ¢ç´¢ç‡
        agent.update_epsilon()
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        scores.append(score)
        scores_window.append(score)
        episode_duration = time.time() - episode_start_time
        episode_durations.append(episode_duration)
        
        # æ›´æ–°å•ä¸ªå›åˆæœ€ä½³åˆ†æ•°
        if score > best_score:
            best_score = score
        
        # è®¡ç®—å½“å‰å¹³å‡åˆ†æ•°ï¼ˆç”¨äºæ˜¾ç¤ºå’Œæ£€æŸ¥ï¼‰
        mean_score = np.mean(scores_window) if len(scores_window) > 0 else 0
        
        # æ¯10å›åˆæ›´æ–°æ˜¾ç¤ºç”¨çš„æœ€ä½³å¹³å‡åˆ†æ•°ï¼ˆä»…ç”¨äºæ˜¾ç¤ºï¼Œä¸ä¿å­˜æ¨¡å‹ï¼‰
        if episode % PRINT_INTERVAL == 0 and len(scores_window) >= BEST_CHECK_INTERVAL:
            if mean_score > display_best_mean_score:
                display_best_mean_score = mean_score
        
        # åœ¨ç¬¬ä¸€æ¬¡æœ‰è¶³å¤Ÿæ•°æ®æ—¶åˆå§‹åŒ–æœ€ä½³å¹³å‡åˆ†æ•°å¹¶ä¿å­˜æ¨¡å‹
        if best_mean_score == -np.inf and len(scores_window) >= BEST_CHECK_INTERVAL:
            best_mean_score = mean_score
            display_best_mean_score = mean_score
            agent.save(BEST_MODEL_PATH)
            print(f"\nğŸ‰ åˆå§‹åŒ–æœ€ä½³å¹³å‡åˆ†æ•°: {best_mean_score:.1f} (å›åˆ {episode}, æœ€è¿‘{len(scores_window)}å›åˆå¹³å‡)")
        
        # æ¯50å›åˆæ£€æŸ¥ä¸€æ¬¡å¹³å‡åˆ†ï¼Œå¦‚æœè¶…è¿‡å†å²æœ€ä½³å¹³å‡åˆ†åˆ™ä¿å­˜ä¸ºæœ€ä½³æ¨¡å‹
        best_model_saved = False
        if episode % BEST_CHECK_INTERVAL == 0 and len(scores_window) >= BEST_CHECK_INTERVAL:
            if mean_score > best_mean_score:
                best_mean_score = mean_score
                display_best_mean_score = mean_score  # åŒæ­¥æ›´æ–°æ˜¾ç¤ºå€¼
                agent.save(BEST_MODEL_PATH)
                best_model_saved = True
                print(f"\nğŸ‰ æ–°çš„æœ€ä½³å¹³å‡åˆ†æ•°: {best_mean_score:.1f} (å›åˆ {episode}, æœ€è¿‘{len(scores_window)}å›åˆå¹³å‡)")
        
        # æ‰“å°è®­ç»ƒä¿¡æ¯
        if episode % PRINT_INTERVAL == 0:
            mean_duration = np.mean(episode_durations[-PRINT_INTERVAL:])
            elapsed_time = time.time() - start_time
            
            # è®¡ç®—å¹³å‡æŸå¤±ï¼ˆæœ€è¿‘1000æ¬¡æ›´æ–°ï¼Œå¦‚æœä¸è¶³1000æ¬¡åˆ™ä½¿ç”¨å…¨éƒ¨ï¼‰
            if len(agent.losses) > 0:
                recent_losses = agent.losses[-1000:] if len(agent.losses) >= 1000 else agent.losses
                avg_loss = np.mean(recent_losses)
                loss_str = f"Avg Loss: {avg_loss:.4f}"
            else:
                loss_str = "Avg Loss: N/A"
            
            # æ ¼å¼åŒ–æœ€ä½³å¹³å‡åˆ†æ•°æ˜¾ç¤ºï¼ˆä½¿ç”¨æ˜¾ç¤ºç”¨çš„æœ€ä½³å¹³å‡åˆ†ï¼Œå¦‚æœè¿˜æ˜¯-infåˆ™æ˜¾ç¤ºå½“å‰å¹³å‡åˆ†ï¼‰
            best_mean_display = display_best_mean_score if display_best_mean_score != -np.inf else mean_score
            
            print(f"å›åˆ {episode:4d} | "
                  f"å¹³å‡åˆ†æ•°: {mean_score:7.1f} | "
                  f"å½“å‰åˆ†æ•°: {score:7.1f} | "
                  f"æœ€ä½³åˆ†æ•°: {best_score:7.1f} | "
                  f"æœ€ä½³å¹³å‡: {best_mean_display:7.1f} | "
                  f"Îµ: {agent.epsilon:.3f} | "
                  f"æ­¥æ•°: {step+1:4d} | "
                  f"æ—¶é—´: {mean_duration:.2f}s | "
                  f"{loss_str}")
        
        # å®šæœŸä¿å­˜æ¨¡å‹checkpointï¼ˆä¸è¦†ç›–å†å²checkpointï¼‰
        if episode % SAVE_INTERVAL == 0:
            checkpoint_path = os.path.join(MODEL_DIR, f"dqn_checkpoint_{episode}.pth")
            agent.save(checkpoint_path)
            if best_model_saved:
                print(f"Checkpointå·²ä¿å­˜: {checkpoint_path} (åŒæ—¶å·²ä¿å­˜æœ€ä½³æ¨¡å‹)")
            else:
                print(f"Checkpointå·²ä¿å­˜: {checkpoint_path}")
    
    # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
    agent.save(MODEL_PATH)
    total_time = time.time() - start_time
    
    print("\n" + "=" * 60)
    print("è®­ç»ƒå®Œæˆï¼")
    print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time/60:.2f} åˆ†é’Ÿ")
    print(f"æœ€ä½³å•å›åˆåˆ†æ•°: {best_score:.1f}")
    print(f"æœ€ä½³å¹³å‡åˆ†æ•°: {best_mean_score:.1f}")
    print(f"æœ€å100å›åˆå¹³å‡åˆ†æ•°: {np.mean(scores_window):.1f}")
    print("=" * 60)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curve(scores, scores_window)
    
    env.close()
    return agent, scores


def plot_training_curve(scores, scores_window):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    try:
        plt.figure(figsize=(12, 5))
        
        # å­å›¾1: æ‰€æœ‰å›åˆçš„åˆ†æ•°
        plt.subplot(1, 2, 1)
        plt.plot(scores, alpha=0.3, color='blue', label='æ¯å›åˆåˆ†æ•°')
        if len(scores_window) > 0:
            window_scores = [np.mean(list(scores_window)[:i+1]) for i in range(len(scores_window))]
            plt.plot(range(len(scores) - len(scores_window) + 1, len(scores) + 1), 
                    window_scores, color='red', linewidth=2, label=f'å¹³å‡åˆ†æ•° ({SCORE_WINDOW}å›åˆ)')
        plt.xlabel('å›åˆ')
        plt.ylabel('åˆ†æ•°')
        plt.title('è®­ç»ƒè¿‡ç¨‹ - åˆ†æ•°æ›²çº¿')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å­å›¾2: æœ€è¿‘100å›åˆçš„å¹³å‡åˆ†æ•°
        plt.subplot(1, 2, 2)
        if len(scores_window) > 0:
            window_scores = list(scores_window)
            plt.plot(window_scores, alpha=0.5, color='blue', label='æœ€è¿‘100å›åˆåˆ†æ•°')
            if len(window_scores) >= 10:
                # ç§»åŠ¨å¹³å‡
                moving_avg = np.convolve(window_scores, np.ones(10)/10, mode='valid')
                plt.plot(range(9, len(window_scores)), moving_avg, 
                        color='red', linewidth=2, label='10å›åˆç§»åŠ¨å¹³å‡')
        plt.xlabel('å›åˆ (æœ€è¿‘100å›åˆ)')
        plt.ylabel('åˆ†æ•°')
        plt.title('æœ€è¿‘100å›åˆåˆ†æ•°è¶‹åŠ¿')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('training_curve.png', dpi=150, bbox_inches='tight')
        print("è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: training_curve.png")
        plt.close()
    except Exception as e:
        print(f"ç»˜åˆ¶è®­ç»ƒæ›²çº¿æ—¶å‡ºé”™: {e}")


def test_agent(model_path=None, n_episodes=5, render=True):
    """
    æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€ä½³æ¨¡å‹
        n_episodes: æµ‹è¯•å›åˆæ•°
        render: æ˜¯å¦æ¸²æŸ“
    """
    if model_path is None:
        model_path = BEST_MODEL_PATH
    
    # åˆ›å»ºç¯å¢ƒ
    render_mode = "human" if render else None
    env = DoodleJumpEnv(render_mode=render_mode)
    
    # åˆ›å»ºæ™ºèƒ½ä½“å¹¶åŠ è½½æ¨¡å‹
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n
    agent = DQNAgent(state_size=state_size, action_size=action_size)
    
    if not agent.load(model_path):
        print("æ— æ³•åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨éšæœºç­–ç•¥")
        return
    
    # è®¾ç½®æ¢ç´¢ç‡ä¸º0ï¼ˆå®Œå…¨è´ªå©ªï¼‰
    agent.epsilon = 0.0
    
    print(f"\nå¼€å§‹æµ‹è¯•æ™ºèƒ½ä½“ (æ¨¡å‹: {model_path})")
    print("=" * 60)
    
    test_scores = []
    
    for episode in range(1, n_episodes + 1):
        state, info = env.reset()
        score = 0
        step = 0
        
        while True:
            # ä½¿ç”¨è´ªå©ªç­–ç•¥ï¼ˆä¸æ¢ç´¢ï¼‰
            action = agent.act(state, training=False)
            state, reward, terminated, truncated, info = env.step(action)
            score = info['score']
            step += 1
            
            if render:
                # å¤„ç†pygameäº‹ä»¶
                import pygame
                for event in pygame.event.get():
                    if event.type == pygame.QUIT:
                        env.close()
                        return
            
            if terminated or truncated:
                break
        
        test_scores.append(score)
        print(f"æµ‹è¯•å›åˆ {episode}: åˆ†æ•° = {score:.1f}, æ­¥æ•° = {step}")
    
    env.close()
    
    print("=" * 60)
    print(f"æµ‹è¯•å®Œæˆï¼")
    print(f"å¹³å‡åˆ†æ•°: {np.mean(test_scores):.1f}")
    print(f"æœ€é«˜åˆ†æ•°: {np.max(test_scores):.1f}")
    print(f"æœ€ä½åˆ†æ•°: {np.min(test_scores):.1f}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='DQNè®­ç»ƒå’Œæµ‹è¯•')
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'],
                       help='è¿è¡Œæ¨¡å¼: train æˆ– test')
    parser.add_argument('--model', type=str, default=None,
                       help='æµ‹è¯•æ—¶ä½¿ç”¨çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=5,
                       help='æµ‹è¯•å›åˆæ•°')
    parser.add_argument('--no-render', action='store_true',
                       help='æµ‹è¯•æ—¶ä¸æ¸²æŸ“')
    
    args = parser.parse_args()
    
    if args.mode == 'train':
        train()
    elif args.mode == 'test':
        test_agent(model_path=args.model, n_episodes=args.episodes, render=not args.no_render)

